{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_ReconstructionCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzQbVsleVDJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1cb1d4d-d870-4a89-836d-4087f81924c4"
      },
      "source": [
        "\n",
        "\n",
        "########## DATASET LICENSE INFO #############\n",
        "\n",
        "''' \n",
        "Flickr-Faces-HQ (FFHQ) is a high-quality image dataset of human faces,\n",
        "originally created as a benchmark for generative adversarial networks (GAN):\n",
        "\n",
        "    A Style-Based Generator Architecture for Generative Adversarial Networks\n",
        "    Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA)\n",
        "    http://stylegan.xyz/paper\n",
        "\n",
        "The individual images were published in Flickr by their respective authors\n",
        "under either Creative Commons BY 2.0, Creative Commons BY-NC 2.0,\n",
        "Public Domain Mark 1.0, Public Domain CC0 1.0, or U.S. Government Works\n",
        "license. All of these licenses allow free use, redistribution, and adaptation\n",
        "for non-commercial purposes. However, some of them require giving appropriate\n",
        "credit to the original author, as well as indicating any changes that were\n",
        "made to the images. The license and original author of each image are\n",
        "indicated in the metadata.\n",
        "\n",
        "    https://creativecommons.org/licenses/by/2.0/\n",
        "    https://creativecommons.org/licenses/by-nc/2.0/\n",
        "    https://creativecommons.org/publicdomain/mark/1.0/\n",
        "    https://creativecommons.org/publicdomain/zero/1.0/\n",
        "    http://www.usa.gov/copyright.shtml\n",
        "\n",
        "The dataset itself (including JSON metadata, download script, and\n",
        "documentation) is made available under Creative Commons BY-NC-SA 4.0 license\n",
        "by NVIDIA Corporation. You can use, redistribute, and adapt it for\n",
        "non-commercial purposes, as long as you (a) give appropriate credit by\n",
        "citing our paper, (b) indicate any changes that you've made, and\n",
        "(c) distribute any derivative works under the same license.\n",
        "\n",
        "    https://creativecommons.org/licenses/by-nc-sa/4.0/\n",
        "'''\n",
        "\n",
        "###################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "from tensorflow.python.framework import ops\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "latent_dim = 64 \n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "img_dims = (128,128,3)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "\n",
        "x_train = []\n",
        "i = 0\n",
        "directory = '/content/gdrive/My Drive/Unsupervised_Faces/'\n",
        "folder = \"000\"\n",
        "\n",
        "\n",
        "\n",
        "class CNN(Model):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Input(shape=(128, 128, 1)),\n",
        "      layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
        "      layers.Dropout(0.3),\n",
        "      layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides=2),\n",
        "      layers.Dropout(0.3),\n",
        "      layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
        "      layers.Dropout(0.3),\n",
        "      layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)\n",
        "      ])\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      layers.Dropout(0.3),\n",
        "      layers.Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      layers.Dropout(0.4),\n",
        "      layers.Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      layers.Dropout(0.5),\n",
        "      layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "def rounded_accuracy(y_true, y_pred):\n",
        "    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n",
        "\n",
        "\n",
        "\n",
        "while i != 51:\n",
        "  print(\"folder \" + str(i) + \" out of 50\\n\")\n",
        "\n",
        "  if i < 10:\n",
        "    for filename in glob.iglob(os.path.join(directory + \"0\" + str(i) + folder, \"*.png\")):\n",
        "      img = cv2.imread(filename)#Image.open(filename)\n",
        "      img = cv2.resize(img, (img_dims[0],img_dims[1]))\n",
        "      img = img_to_array(img) #tf.keras.preprocessing.image.img_to_array(im)\n",
        "      img = tf.image.rgb_to_grayscale(img)\n",
        "      x_train.append(img)\n",
        "\n",
        "  else:\n",
        "    for filename in glob.iglob(os.path.join(directory +  str(i) + folder, \"*.png\")):\n",
        "      img = cv2.imread(filename)#Image.open(filename)\n",
        "      img = cv2.resize(img, (img_dims[0],img_dims[1]))\n",
        "      img = img_to_array(img) #tf.keras.preprocessing.image.img_to_array(im)\n",
        "      img = tf.image.rgb_to_grayscale(img)\n",
        "      x_train.append(img)   \n",
        "\n",
        "  i += 1\n",
        "\n",
        "print('Done!')\n",
        "\n",
        "#autoencoder = Autoencoder(latent_dim)\n",
        "\n",
        "\n",
        "X_train = np.array(x_train, dtype=\"float\") / 255.00\n",
        "X_test = np.array(x_train, dtype=\"float\") / 255.00\n",
        "X_train, X_valid = X_train[:-300], X_train[-300:]\n",
        "\n",
        "X_train = tf.convert_to_tensor(X_train, dtype=tf.int32)\n",
        "X_valid = tf.convert_to_tensor(X_valid, dtype=tf.int32)\n",
        "\n",
        "\n",
        "model = CNN()\n",
        "model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError())\n",
        "model.fit(X_train, X_train, epochs=20, validation_data=(X_test,X_test))\n",
        "model.save('/content/gdrive/My Drive/UnsupCNN')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "folder 0 out of 50\n",
            "\n",
            "folder 1 out of 50\n",
            "\n",
            "folder 2 out of 50\n",
            "\n",
            "folder 3 out of 50\n",
            "\n",
            "folder 4 out of 50\n",
            "\n",
            "folder 5 out of 50\n",
            "\n",
            "folder 6 out of 50\n",
            "\n",
            "folder 7 out of 50\n",
            "\n",
            "folder 8 out of 50\n",
            "\n",
            "folder 9 out of 50\n",
            "\n",
            "folder 10 out of 50\n",
            "\n",
            "folder 11 out of 50\n",
            "\n",
            "folder 12 out of 50\n",
            "\n",
            "folder 13 out of 50\n",
            "\n",
            "folder 14 out of 50\n",
            "\n",
            "folder 15 out of 50\n",
            "\n",
            "folder 16 out of 50\n",
            "\n",
            "folder 17 out of 50\n",
            "\n",
            "folder 18 out of 50\n",
            "\n",
            "folder 19 out of 50\n",
            "\n",
            "folder 20 out of 50\n",
            "\n",
            "folder 21 out of 50\n",
            "\n",
            "folder 22 out of 50\n",
            "\n",
            "folder 23 out of 50\n",
            "\n",
            "folder 24 out of 50\n",
            "\n",
            "folder 25 out of 50\n",
            "\n",
            "folder 26 out of 50\n",
            "\n",
            "folder 27 out of 50\n",
            "\n",
            "folder 28 out of 50\n",
            "\n",
            "folder 29 out of 50\n",
            "\n",
            "folder 30 out of 50\n",
            "\n",
            "folder 31 out of 50\n",
            "\n",
            "folder 32 out of 50\n",
            "\n",
            "folder 33 out of 50\n",
            "\n",
            "folder 34 out of 50\n",
            "\n",
            "folder 35 out of 50\n",
            "\n",
            "folder 36 out of 50\n",
            "\n",
            "folder 37 out of 50\n",
            "\n",
            "folder 38 out of 50\n",
            "\n",
            "folder 39 out of 50\n",
            "\n",
            "folder 40 out of 50\n",
            "\n",
            "folder 41 out of 50\n",
            "\n",
            "folder 42 out of 50\n",
            "\n",
            "folder 43 out of 50\n",
            "\n",
            "folder 44 out of 50\n",
            "\n",
            "folder 45 out of 50\n",
            "\n",
            "folder 46 out of 50\n",
            "\n",
            "folder 47 out of 50\n",
            "\n",
            "folder 48 out of 50\n",
            "\n",
            "folder 49 out of 50\n",
            "\n",
            "folder 50 out of 50\n",
            "\n",
            "Done!\n",
            "Epoch 1/20\n",
            "1486/1486 [==============================] - 2214s 1s/step - loss: 0.1279 - val_loss: 0.0975\n",
            "Epoch 2/20\n",
            "1486/1486 [==============================] - 2237s 2s/step - loss: 0.0322 - val_loss: 0.1475\n",
            "Epoch 3/20\n",
            "1486/1486 [==============================] - 2222s 1s/step - loss: 0.0111 - val_loss: 0.1787\n",
            "Epoch 4/20\n",
            "1486/1486 [==============================] - 2223s 1s/step - loss: 0.0046 - val_loss: 0.1985\n",
            "Epoch 5/20\n",
            "1486/1486 [==============================] - 2222s 1s/step - loss: 0.0020 - val_loss: 0.2117\n",
            "Epoch 6/20\n",
            "1486/1486 [==============================] - 2202s 1s/step - loss: 9.5331e-04 - val_loss: 0.2207\n",
            "Epoch 7/20\n",
            "1486/1486 [==============================] - 2171s 1s/step - loss: 4.5546e-04 - val_loss: 0.2269\n",
            "Epoch 8/20\n",
            "1486/1486 [==============================] - 2428s 2s/step - loss: 2.2053e-04 - val_loss: 0.2312\n",
            "Epoch 9/20\n",
            "1486/1486 [==============================] - 2240s 2s/step - loss: 1.0764e-04 - val_loss: 0.2343\n",
            "Epoch 10/20\n",
            "1486/1486 [==============================] - 2172s 1s/step - loss: 5.2807e-05 - val_loss: 0.2364\n",
            "Epoch 11/20\n",
            "1486/1486 [==============================] - 2202s 1s/step - loss: 2.5991e-05 - val_loss: 0.2379\n",
            "Epoch 12/20\n",
            "1486/1486 [==============================] - 2205s 1s/step - loss: 1.2822e-05 - val_loss: 0.2389\n",
            "Epoch 13/20\n",
            "1486/1486 [==============================] - 2175s 1s/step - loss: 6.3358e-06 - val_loss: 0.2397\n",
            "Epoch 14/20\n",
            "1486/1486 [==============================] - 2245s 2s/step - loss: 3.1355e-06 - val_loss: 0.2402\n",
            "Epoch 15/20\n",
            "1486/1486 [==============================] - 2314s 2s/step - loss: 1.5543e-06 - val_loss: 0.2406\n",
            "Epoch 16/20\n",
            "1486/1486 [==============================] - 2441s 2s/step - loss: 7.7244e-07 - val_loss: 0.2408\n",
            "Epoch 17/20\n",
            "1486/1486 [==============================] - 2470s 2s/step - loss: 3.8556e-07 - val_loss: 0.2410\n",
            "Epoch 18/20\n",
            "1486/1486 [==============================] - 2553s 2s/step - loss: 1.9402e-07 - val_loss: 0.2411\n",
            "Epoch 19/20\n",
            "1486/1486 [==============================] - 2619s 2s/step - loss: 9.9141e-08 - val_loss: 0.2412\n",
            "Epoch 20/20\n",
            "1486/1486 [==============================] - 2452s 2s/step - loss: 5.2063e-08 - val_loss: 0.2413\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/UnsupCNN/assets\n"
          ]
        }
      ]
    }
  ]
}